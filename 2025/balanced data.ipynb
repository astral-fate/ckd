{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdd64b1-1cfa-4496-bd01-d1ffeced1b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic dataset with 50000 samples (10000 per stage)...\n",
      "\n",
      "Dataset Summary:\n",
      "Total samples: 50000\n",
      "\n",
      "CKD Stage distribution:\n",
      "CKD_Stage\n",
      "Stage 5     10000\n",
      "Stage 4     10000\n",
      "Stage 2     10000\n",
      "Stage 1     10000\n",
      "Stage 3b     5000\n",
      "Stage 3a     5000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Albuminuria Category distribution:\n",
      "Albuminuria_Category\n",
      "A1    21899\n",
      "A3    15103\n",
      "A2    12998\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Risk Category distribution:\n",
      "Risk_Category\n",
      "Very high               24485\n",
      "Low                     15014\n",
      "Moderately increased     5957\n",
      "High                     4544\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset split:\n",
      "Training set: 30000 samples (60.0%)\n",
      "Validation set: 10000 samples (20.0%)\n",
      "Test set: 10000 samples (20.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Synthetic Dataset Generator for Multi-Class CKD Classification\n",
    "with Strict Balance Across All Stages and Comprehensive Biomarker Panel\n",
    "\n",
    "Based on KDIGO 2024 Guidelines\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import beta, norm, lognorm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Create output directory for visualizations\n",
    "os.makedirs('./visualizations', exist_ok=True)\n",
    "\n",
    "def generate_synthetic_dataset(n_samples_per_stage=10000, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset for CKD classification with strict balance across all stages\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples_per_stage : int\n",
    "        Number of samples per CKD stage (total samples will be 5 * n_samples_per_stage)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Synthetic dataset with demographic information, biomarkers, and CKD stages\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Total number of samples\n",
    "    n_total = n_samples_per_stage * 5\n",
    "    \n",
    "    print(f\"Generating synthetic dataset with {n_total} samples ({n_samples_per_stage} per stage)...\")\n",
    "    \n",
    "    # Initialize empty dataframe\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    # Generate demographic data\n",
    "    data['Age'] = np.random.normal(60, 15, n_total)\n",
    "    data['Age'] = np.clip(data['Age'], 18, 95).round().astype(int)\n",
    "    \n",
    "    # Gender (0 = Female, 1 = Male)\n",
    "    data['Gender'] = np.random.binomial(1, 0.5, n_total)\n",
    "    \n",
    "    # Race/Ethnicity (simplified for demonstration)\n",
    "    # 0 = White, 1 = Black, 2 = Hispanic, 3 = Asian, 4 = Other\n",
    "    race_probs = [0.65, 0.12, 0.15, 0.06, 0.02]\n",
    "    data['Race_Ethnicity'] = np.random.choice(5, n_total, p=race_probs)\n",
    "    \n",
    "    # BMI\n",
    "    data['BMI'] = np.random.normal(27, 5, n_total)\n",
    "    data['BMI'] = np.clip(data['BMI'], 15, 45)\n",
    "    \n",
    "    # Generate CKD stages with strict balance\n",
    "    stages = ['Stage 1', 'Stage 2', 'Stage 3a', 'Stage 3b', 'Stage 4', 'Stage 5']\n",
    "    stage_values = []\n",
    "    \n",
    "    for stage in range(1, 6):\n",
    "        if stage == 3:\n",
    "            # Split Stage 3 into 3a and 3b\n",
    "            stage_values.extend(['Stage 3a'] * (n_samples_per_stage // 2))\n",
    "            stage_values.extend(['Stage 3b'] * (n_samples_per_stage // 2))\n",
    "        else:\n",
    "            stage_values.extend([f'Stage {stage}'] * n_samples_per_stage)\n",
    "    \n",
    "    # Shuffle stage values\n",
    "    np.random.shuffle(stage_values)\n",
    "    data['CKD_Stage'] = stage_values\n",
    "    \n",
    "    # Generate GFR values based on CKD stage\n",
    "    data['GFR'] = np.nan\n",
    "    \n",
    "    # Define GFR ranges for each stage\n",
    "    gfr_ranges = {\n",
    "        'Stage 1': (90, 150),\n",
    "        'Stage 2': (60, 89),\n",
    "        'Stage 3a': (45, 59),\n",
    "        'Stage 3b': (30, 44),\n",
    "        'Stage 4': (15, 29),\n",
    "        'Stage 5': (1, 14)\n",
    "    }\n",
    "    \n",
    "    # Generate GFR values using beta distribution to create realistic clustering\n",
    "    for stage, (min_gfr, max_gfr) in gfr_ranges.items():\n",
    "        mask = data['CKD_Stage'] == stage\n",
    "        n_stage = mask.sum()\n",
    "        \n",
    "        if stage == 'Stage 1':\n",
    "            # For Stage 1, use a right-skewed distribution\n",
    "            alpha, beta_param = 2, 5\n",
    "            gfr_values = beta.rvs(alpha, beta_param, size=n_stage)\n",
    "            # Scale to the GFR range\n",
    "            gfr_values = min_gfr + gfr_values * (max_gfr - min_gfr)\n",
    "        elif stage == 'Stage 5':\n",
    "            # For Stage 5, use a left-skewed distribution\n",
    "            alpha, beta_param = 5, 2\n",
    "            gfr_values = beta.rvs(alpha, beta_param, size=n_stage)\n",
    "            # Scale to the GFR range\n",
    "            gfr_values = min_gfr + gfr_values * (max_gfr - min_gfr)\n",
    "        else:\n",
    "            # For other stages, use a symmetric beta distribution\n",
    "            alpha = beta_param = 2\n",
    "            gfr_values = beta.rvs(alpha, beta_param, size=n_stage)\n",
    "            # Scale to the GFR range\n",
    "            gfr_values = min_gfr + gfr_values * (max_gfr - min_gfr)\n",
    "        \n",
    "        data.loc[mask, 'GFR'] = gfr_values\n",
    "    \n",
    "    # Round GFR to 1 decimal place\n",
    "    data['GFR'] = data['GFR'].round(1)\n",
    "    \n",
    "    # Generate albuminuria categories (A1, A2, A3) based on CKD stage\n",
    "    # A1: <30 mg/g, A2: 30-300 mg/g, A3: >300 mg/g\n",
    "    data['Albuminuria_Category'] = ''\n",
    "    \n",
    "    # Define probabilities of albuminuria categories for each stage\n",
    "    # Format: [P(A1), P(A2), P(A3)]\n",
    "    albuminuria_probs = {\n",
    "        'Stage 1': [0.8, 0.15, 0.05],\n",
    "        'Stage 2': [0.7, 0.2, 0.1],\n",
    "        'Stage 3a': [0.5, 0.3, 0.2],\n",
    "        'Stage 3b': [0.3, 0.4, 0.3],\n",
    "        'Stage 4': [0.2, 0.3, 0.5],\n",
    "        'Stage 5': [0.1, 0.3, 0.6]\n",
    "    }\n",
    "    \n",
    "    for stage, probs in albuminuria_probs.items():\n",
    "        mask = data['CKD_Stage'] == stage\n",
    "        n_stage = mask.sum()\n",
    "        \n",
    "        categories = np.random.choice(['A1', 'A2', 'A3'], size=n_stage, p=probs)\n",
    "        data.loc[mask, 'Albuminuria_Category'] = categories\n",
    "    \n",
    "    # Generate ACR values based on albuminuria category\n",
    "    data['ACR'] = np.nan\n",
    "    \n",
    "    # Define ACR ranges for each category\n",
    "    acr_ranges = {\n",
    "        'A1': (0, 29),\n",
    "        'A2': (30, 300),\n",
    "        'A3': (301, 5000)\n",
    "    }\n",
    "    \n",
    "    # Generate ACR values\n",
    "    for category, (min_acr, max_acr) in acr_ranges.items():\n",
    "        mask = data['Albuminuria_Category'] == category\n",
    "        n_category = mask.sum()\n",
    "        \n",
    "        if category == 'A1':\n",
    "            # For A1, use a right-skewed distribution\n",
    "            shape, loc, scale = 1.5, 0, 10\n",
    "            acr_values = lognorm.rvs(shape, loc, scale, size=n_category)\n",
    "            acr_values = np.clip(acr_values, 0, min_acr)\n",
    "        elif category == 'A3':\n",
    "            # For A3, use a right-skewed distribution with higher values\n",
    "            shape, loc, scale = 1.2, max_acr, 1000\n",
    "            acr_values = lognorm.rvs(shape, loc, scale, size=n_category)\n",
    "            acr_values = np.clip(acr_values, max_acr, 5000)\n",
    "        else:\n",
    "            # For A2, use a more uniform distribution\n",
    "            acr_values = np.random.uniform(min_acr, max_acr, n_category)\n",
    "        \n",
    "        data.loc[mask, 'ACR'] = acr_values\n",
    "    \n",
    "    # Round ACR to nearest integer\n",
    "    data['ACR'] = data['ACR'].round().astype(int)\n",
    "    \n",
    "    # Generate comorbidities\n",
    "    # Diabetes (0 = No, 1 = Yes)\n",
    "    diabetes_probs = {\n",
    "        'Stage 1': 0.2,\n",
    "        'Stage 2': 0.3,\n",
    "        'Stage 3a': 0.4,\n",
    "        'Stage 3b': 0.45,\n",
    "        'Stage 4': 0.5,\n",
    "        'Stage 5': 0.55\n",
    "    }\n",
    "    \n",
    "    data['Diabetes'] = 0\n",
    "    for stage, prob in diabetes_probs.items():\n",
    "        mask = data['CKD_Stage'] == stage\n",
    "        n_stage = mask.sum()\n",
    "        data.loc[mask, 'Diabetes'] = np.random.binomial(1, prob, n_stage)\n",
    "    \n",
    "    # Hypertension (0 = No, 1 = Yes)\n",
    "    hypertension_probs = {\n",
    "        'Stage 1': 0.3,\n",
    "        'Stage 2': 0.5,\n",
    "        'Stage 3a': 0.7,\n",
    "        'Stage 3b': 0.8,\n",
    "        'Stage 4': 0.85,\n",
    "        'Stage 5': 0.9\n",
    "    }\n",
    "    \n",
    "    data['Hypertension'] = 0\n",
    "    for stage, prob in hypertension_probs.items():\n",
    "        mask = data['CKD_Stage'] == stage\n",
    "        n_stage = mask.sum()\n",
    "        data.loc[mask, 'Hypertension'] = np.random.binomial(1, prob, n_stage)\n",
    "    \n",
    "    # Generate creatinine based on GFR, age, gender\n",
    "    # Using reversed CKD-EPI 2021 equation\n",
    "    data['Creatinine'] = np.nan\n",
    "    \n",
    "    # Constants for CKD-EPI 2021 equation\n",
    "    k = np.where(data['Gender'] == 0, 0.7, 0.9)  # Female = 0.7, Male = 0.9\n",
    "    alpha = np.where(data['Gender'] == 0, -0.241, -0.302)  # Female = -0.241, Male = -0.302\n",
    "    \n",
    "    # Calculate creatinine based on GFR\n",
    "    # SCr = k * (GFR / (142 * 0.9938^Age))^(1/alpha)\n",
    "    data['Creatinine'] = k * (data['GFR'] / (142 * 0.9938**data['Age']))**(1/alpha)\n",
    "    \n",
    "    # Add some biological variation\n",
    "    variation = np.random.normal(0, 0.1, n_total)\n",
    "    data['Creatinine'] = data['Creatinine'] * (1 + variation)\n",
    "    \n",
    "    # Ensure creatinine is within realistic ranges\n",
    "    data['Creatinine'] = np.clip(data['Creatinine'], 0.2, 15.0)\n",
    "    \n",
    "    # Round creatinine to 2 decimal places\n",
    "    data['Creatinine'] = data['Creatinine'].round(2)\n",
    "    \n",
    "    # Generate cystatin C based on GFR\n",
    "    # Using reversed CKD-EPI 2012 cystatin C equation\n",
    "    data['Cystatin_C'] = np.nan\n",
    "    \n",
    "    # Calculate cystatin C based on GFR\n",
    "    # SCys = 0.8 * (GFR / (133 * 0.996^Age * (0.932 if female)))^(1/-0.499)\n",
    "    female_factor = np.where(data['Gender'] == 0, 0.932, 1.0)\n",
    "    data['Cystatin_C'] = 0.8 * (data['GFR'] / (133 * 0.996**data['Age'] * female_factor))**(1/-0.499)\n",
    "    \n",
    "    # Add some biological variation\n",
    "    variation = np.random.normal(0, 0.1, n_total)\n",
    "    data['Cystatin_C'] = data['Cystatin_C'] * (1 + variation)\n",
    "    \n",
    "    # Ensure cystatin C is within realistic ranges\n",
    "    data['Cystatin_C'] = np.clip(data['Cystatin_C'], 0.4, 8.0)\n",
    "    \n",
    "    # Round cystatin C to 2 decimal places\n",
    "    data['Cystatin_C'] = data['Cystatin_C'].round(2)\n",
    "    \n",
    "    # Generate blood urea based on GFR\n",
    "    # Blood urea increases as GFR decreases\n",
    "    data['Blood_Urea'] = 40 * np.exp(-0.02 * data['GFR']) + np.random.normal(0, 5, n_total)\n",
    "    \n",
    "    # Adjust for age (slight increase with age)\n",
    "    data['Blood_Urea'] += (data['Age'] - 60) * 0.1\n",
    "    \n",
    "    # Ensure blood urea is within realistic ranges\n",
    "    data['Blood_Urea'] = np.clip(data['Blood_Urea'], 5, 200)\n",
    "    \n",
    "    # Round blood urea to nearest integer\n",
    "    data['Blood_Urea'] = data['Blood_Urea'].round().astype(int)\n",
    "    \n",
    "    # Generate hemoglobin based on GFR and gender\n",
    "    # Hemoglobin decreases as GFR decreases, and is higher in males\n",
    "    base_hgb = np.where(data['Gender'] == 1, 14.5, 13.0)  # Base hemoglobin (male/female)\n",
    "    data['Hemoglobin'] = base_hgb + 0.03 * data['GFR'] + np.random.normal(0, 0.8, n_total)\n",
    "    \n",
    "    # Ensure hemoglobin is within realistic ranges\n",
    "    data['Hemoglobin'] = np.clip(data['Hemoglobin'], 5, 18)\n",
    "    \n",
    "    # Round hemoglobin to 1 decimal place\n",
    "    data['Hemoglobin'] = data['Hemoglobin'].round(1)\n",
    "    \n",
    "    # Generate electrolytes\n",
    "    \n",
    "    # Potassium: tends to increase in advanced CKD\n",
    "    data['Potassium'] = 4.0 + 0.5 * np.exp(-0.03 * data['GFR']) + np.random.normal(0, 0.3, n_total)\n",
    "    data['Potassium'] = np.clip(data['Potassium'], 2.5, 7.0)\n",
    "    data['Potassium'] = data['Potassium'].round(1)\n",
    "    \n",
    "    # Sodium: more variable in advanced CKD\n",
    "    data['Sodium'] = 140 + np.random.normal(0, 2 + 0.05 * (60 - data['GFR']).clip(0, 60))\n",
    "    data['Sodium'] = np.clip(data['Sodium'], 125, 150)\n",
    "    data['Sodium'] = data['Sodium'].round().astype(int)\n",
    "    \n",
    "    # Calcium: tends to decrease in advanced CKD\n",
    "    data['Calcium'] = 9.5 - 0.5 * np.exp(-0.04 * data['GFR']) + np.random.normal(0, 0.3, n_total)\n",
    "    data['Calcium'] = np.clip(data['Calcium'], 6.0, 11.0)\n",
    "    data['Calcium'] = data['Calcium'].round(1)\n",
    "    \n",
    "    # Phosphate: tends to increase in advanced CKD\n",
    "    data['Phosphate'] = 3.5 + 2.0 * np.exp(-0.05 * data['GFR']) + np.random.normal(0, 0.5, n_total)\n",
    "    data['Phosphate'] = np.clip(data['Phosphate'], 2.0, 10.0)\n",
    "    data['Phosphate'] = data['Phosphate'].round(1)\n",
    "    \n",
    "    # Bicarbonate: decreases in advanced CKD (metabolic acidosis)\n",
    "    data['Bicarbonate'] = 24 - 8 * np.exp(-0.06 * data['GFR']) + np.random.normal(0, 1.5, n_total)\n",
    "    data['Bicarbonate'] = np.clip(data['Bicarbonate'], 10, 32)\n",
    "    data['Bicarbonate'] = data['Bicarbonate'].round().astype(int)\n",
    "    \n",
    "    # Generate additional biomarkers\n",
    "    \n",
    "    # Albumin: tends to decrease in advanced CKD and with proteinuria\n",
    "    albumin_reduction = 0.5 * np.exp(-0.05 * data['GFR'])\n",
    "    albumin_reduction += 0.3 * (data['Albuminuria_Category'] == 'A3').astype(int)\n",
    "    albumin_reduction += 0.2 * (data['Albuminuria_Category'] == 'A2').astype(int)\n",
    "    data['Albumin'] = 4.2 - albumin_reduction + np.random.normal(0, 0.2, n_total)\n",
    "    data['Albumin'] = np.clip(data['Albumin'], 1.5, 5.0)\n",
    "    data['Albumin'] = data['Albumin'].round(1)\n",
    "    \n",
    "    # Uric acid: increases in CKD\n",
    "    data['Uric_Acid'] = 5 + 3 * np.exp(-0.03 * data['GFR']) + np.random.normal(0, 0.8, n_total)\n",
    "    data['Uric_Acid'] = np.clip(data['Uric_Acid'], 2.0, 12.0)\n",
    "    data['Uric_Acid'] = data['Uric_Acid'].round(1)\n",
    "    \n",
    "    # PTH: increases exponentially as GFR decreases\n",
    "    data['PTH'] = 35 + 200 * np.exp(-0.04 * data['GFR']) + np.random.normal(0, 20, n_total)\n",
    "    data['PTH'] = np.clip(data['PTH'], 10, 2000)\n",
    "    data['PTH'] = data['PTH'].round().astype(int)\n",
    "    \n",
    "    # Vitamin D: decreases as GFR decreases\n",
    "    data['Vitamin_D'] = 30 + 0.2 * data['GFR'] + np.random.normal(0, 5, n_total)\n",
    "    data['Vitamin_D'] = np.clip(data['Vitamin_D'], 5, 50)\n",
    "    data['Vitamin_D'] = data['Vitamin_D'].round().astype(int)\n",
    "    \n",
    "    # KIM-1: increases as GFR decreases\n",
    "    data['KIM_1'] = 0.5 + 5 * np.exp(-0.05 * data['GFR']) + np.random.normal(0, 0.5, n_total)\n",
    "    data['KIM_1'] = np.clip(data['KIM_1'], 0.1, 10.0)\n",
    "    data['KIM_1'] = data['KIM_1'].round(2)\n",
    "    \n",
    "    # NGAL: increases as GFR decreases\n",
    "    data['NGAL'] = 20 + 500 * np.exp(-0.05 * data['GFR']) + np.random.normal(0, 50, n_total)\n",
    "    data['NGAL'] = np.clip(data['NGAL'], 10, 1000)\n",
    "    data['NGAL'] = data['NGAL'].round().astype(int)\n",
    "    \n",
    "    # Beta-2 Microglobulin: increases as GFR decreases\n",
    "    data['Beta_2_Microglobulin'] = 2 + 20 * np.exp(-0.04 * data['GFR']) + np.random.normal(0, 1, n_total)\n",
    "    data['Beta_2_Microglobulin'] = np.clip(data['Beta_2_Microglobulin'], 1, 50)\n",
    "    data['Beta_2_Microglobulin'] = data['Beta_2_Microglobulin'].round(1)\n",
    "    \n",
    "    # Create combined CKD stage (GFR + Albuminuria)\n",
    "    data['CKD_Stage_Combined'] = data['CKD_Stage'] + '-' + data['Albuminuria_Category']\n",
    "    \n",
    "    # Create simplified CKD stage (1-5)\n",
    "    stage_mapping = {\n",
    "        'Stage 1': 1,\n",
    "        'Stage 2': 2,\n",
    "        'Stage 3a': 3,\n",
    "        'Stage 3b': 3,\n",
    "        'Stage 4': 4,\n",
    "        'Stage 5': 5\n",
    "    }\n",
    "    data['CKD_Stage_Simple'] = data['CKD_Stage'].map(stage_mapping)\n",
    "    \n",
    "    # Create risk category based on KDIGO heat map\n",
    "    risk_matrix = {\n",
    "        # Format: (GFR Stage, Albuminuria Category): Risk Category\n",
    "        ('Stage 1', 'A1'): 'Low',\n",
    "        ('Stage 1', 'A2'): 'Moderately increased',\n",
    "        ('Stage 1', 'A3'): 'High',\n",
    "        ('Stage 2', 'A1'): 'Low',\n",
    "        ('Stage 2', 'A2'): 'Moderately increased',\n",
    "        ('Stage 2', 'A3'): 'High',\n",
    "        ('Stage 3a', 'A1'): 'Moderately increased',\n",
    "        ('Stage 3a', 'A2'): 'High',\n",
    "        ('Stage 3a', 'A3'): 'Very high',\n",
    "        ('Stage 3b', 'A1'): 'High',\n",
    "        ('Stage 3b', 'A2'): 'Very high',\n",
    "        ('Stage 3b', 'A3'): 'Very high',\n",
    "        ('Stage 4', 'A1'): 'Very high',\n",
    "        ('Stage 4', 'A2'): 'Very high',\n",
    "        ('Stage 4', 'A3'): 'Very high',\n",
    "        ('Stage 5', 'A1'): 'Very high',\n",
    "        ('Stage 5', 'A2'): 'Very high',\n",
    "        ('Stage 5', 'A3'): 'Very high'\n",
    "    }\n",
    "    \n",
    "    data['Risk_Category'] = data.apply(lambda row: risk_matrix[(row['CKD_Stage'], row['Albuminuria_Category'])], axis=1)\n",
    "    \n",
    "    # Visualize the dataset\n",
    "    \n",
    "    # Plot GFR distribution by CKD stage\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='CKD_Stage', y='GFR', data=data)\n",
    "    plt.title('GFR Distribution by CKD Stage')\n",
    "    plt.xlabel('CKD Stage')\n",
    "    plt.ylabel('GFR (mL/min/1.73m²)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./visualizations/gfr_by_stage.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot creatinine vs GFR\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='GFR', y='Creatinine', hue='CKD_Stage', data=data, alpha=0.5)\n",
    "    plt.title('Creatinine vs GFR')\n",
    "    plt.xlabel('GFR (mL/min/1.73m²)')\n",
    "    plt.ylabel('Creatinine (mg/dL)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./visualizations/creatinine_vs_gfr.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot cystatin C vs GFR\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='GFR', y='Cystatin_C', hue='CKD_Stage', data=data, alpha=0.5)\n",
    "    plt.title('Cystatin C vs GFR')\n",
    "    plt.xlabel('GFR (mL/min/1.73m²)')\n",
    "    plt.ylabel('Cystatin C (mg/L)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./visualizations/cystatin_c_vs_gfr.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot albuminuria distribution by CKD stage\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    albuminuria_counts = pd.crosstab(data['CKD_Stage'], data['Albuminuria_Category'])\n",
    "    albuminuria_props = albuminuria_counts.div(albuminuria_counts.sum(axis=1), axis=0)\n",
    "    albuminuria_props.plot(kind='bar', stacked=True)\n",
    "    plt.title('Albuminuria Distribution by CKD Stage')\n",
    "    plt.xlabel('CKD Stage')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./visualizations/albuminuria_by_stage.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot risk category distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    risk_counts = data['Risk_Category'].value_counts()\n",
    "    risk_counts.plot(kind='bar')\n",
    "    plt.title('Risk Category Distribution')\n",
    "    plt.xlabel('Risk Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./visualizations/risk_category_distribution.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Print dataset summary\n",
    "    print(\"\\nDataset Summary:\")\n",
    "    print(f\"Total samples: {len(data)}\")\n",
    "    print(\"\\nCKD Stage distribution:\")\n",
    "    print(data['CKD_Stage'].value_counts())\n",
    "    print(\"\\nAlbuminuria Category distribution:\")\n",
    "    print(data['Albuminuria_Category'].value_counts())\n",
    "    print(\"\\nRisk Category distribution:\")\n",
    "    print(data['Risk_Category'].value_counts())\n",
    "    \n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # First split: 80% train+val, 20% test\n",
    "    train_val_data, test_data = train_test_split(\n",
    "        data, test_size=0.2, stratify=data['CKD_Stage'], random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Second split: 75% train, 25% val (60% train, 20% val of original dataset)\n",
    "    train_data, val_data = train_test_split(\n",
    "        train_val_data, test_size=0.25, stratify=train_val_data['CKD_Stage'], random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Save datasets\n",
    "    train_data.to_csv('ckd_train_data.csv', index=False)\n",
    "    val_data.to_csv('ckd_val_data.csv', index=False)\n",
    "    test_data.to_csv('ckd_test_data.csv', index=False)\n",
    "    \n",
    "    print(\"\\nDataset split:\")\n",
    "    print(f\"Training set: {len(train_data)} samples ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "    print(f\"Validation set: {len(val_data)} samples ({len(val_data)/len(data)*100:.1f}%)\")\n",
    "    print(f\"Test set: {len(test_data)} samples ({len(test_data)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate dataset with 10,000 samples per stage (50,000 total)\n",
    "    data = generate_synthetic_dataset(n_samples_per_stage=10000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886345da-ab75-4fe5-8909-fdc4848389a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5c3d9-8211-4ad2-9e47-5d5746e88d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf042925-a44f-4523-8e4f-ae535ed99921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CKD Classification Pipeline...\n",
      "Data loaded successfully:\n",
      "  Training set: 30000 samples (60%)\n",
      "  Validation set: 10000 samples (20%)\n",
      "  Test set: 10000 samples (20%)\n",
      "  Classes: ['Stage 1', 'Stage 2', 'Stage 3a', 'Stage 3b', 'Stage 4', 'Stage 5']\n",
      "Selected 8 features using hybrid method:\n",
      "['GFR', 'Creatinine', 'Cystatin_C', 'Blood_Urea', 'Phosphate', 'Uric_Acid', 'PTH', 'KIM_1']\n",
      "\n",
      "Training and tuning logistic_regression...\n",
      "Best parameters for logistic_regression: {'classifier__C': 0.01, 'classifier__penalty': None}\n",
      "Best cross-validation score: 0.9987\n",
      "\n",
      "Training and tuning svm...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "os.makedirs('./visualizations', exist_ok=True)\n",
    "\n",
    "class CKDClassificationPipeline:\n",
    "    \"\"\"\n",
    "    Enhanced machine learning pipeline for CKD classification\n",
    "    with advanced models and comprehensive evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the classification pipeline\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.feature_importances = {}\n",
    "        \n",
    "        # Define categorical and numerical features\n",
    "        self.categorical_features = ['Gender', 'Race_Ethnicity', 'Diabetes', 'Hypertension']\n",
    "        self.numerical_features = [\n",
    "            'Age', 'BMI', 'GFR', 'ACR', 'Creatinine', 'Cystatin_C', \n",
    "            'Blood_Urea', 'Hemoglobin', 'Potassium', 'Sodium', 'Calcium', \n",
    "            'Phosphate', 'Bicarbonate', 'Albumin', 'Uric_Acid', 'PTH', 'Vitamin_D',\n",
    "            'KIM_1', 'NGAL', 'Beta_2_Microglobulin'\n",
    "        ]\n",
    "        \n",
    "        # Define target variable\n",
    "        self.target = 'CKD_Stage'\n",
    "    \n",
    "    def load_data(self, train_path, val_path, test_path):\n",
    "        \"\"\"\n",
    "        Load training, validation, and test datasets\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        train_path : str\n",
    "            Path to training dataset\n",
    "        val_path : str\n",
    "            Path to validation dataset\n",
    "        test_path : str\n",
    "            Path to test dataset\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "        \"\"\"\n",
    "        # Load datasets\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        val_data = pd.read_csv(val_path)\n",
    "        test_data = pd.read_csv(test_path)\n",
    "        \n",
    "        # Extract features and target\n",
    "        X_train = train_data.drop([self.target, 'CKD_Stage_Combined', 'CKD_Stage_Simple', \n",
    "                                  'Albuminuria_Category', 'Risk_Category'], axis=1, errors='ignore')\n",
    "        y_train = train_data[self.target]\n",
    "        \n",
    "        X_val = val_data.drop([self.target, 'CKD_Stage_Combined', 'CKD_Stage_Simple', \n",
    "                              'Albuminuria_Category', 'Risk_Category'], axis=1, errors='ignore')\n",
    "        y_val = val_data[self.target]\n",
    "        \n",
    "        X_test = test_data.drop([self.target, 'CKD_Stage_Combined', 'CKD_Stage_Simple', \n",
    "                                'Albuminuria_Category', 'Risk_Category'], axis=1, errors='ignore')\n",
    "        y_test = test_data[self.target]\n",
    "        \n",
    "        # Store data\n",
    "        self.X_train = X_train\n",
    "        self.X_val = X_val\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        # Store class names\n",
    "        self.classes = sorted(y_train.unique())\n",
    "        \n",
    "        print(f\"Data loaded successfully:\")\n",
    "        print(f\"  Training set: {X_train.shape[0]} samples (60%)\")\n",
    "        print(f\"  Validation set: {X_val.shape[0]} samples (20%)\")\n",
    "        print(f\"  Test set: {X_test.shape[0]} samples (20%)\")\n",
    "        print(f\"  Classes: {self.classes}\")\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "    def create_preprocessing_pipeline(self):\n",
    "        \"\"\"\n",
    "        Create preprocessing pipeline for categorical and numerical features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        ColumnTransformer\n",
    "            Preprocessing pipeline\n",
    "        \"\"\"\n",
    "        # Create transformers for categorical and numerical features\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "        \n",
    "        numerical_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        # Combine transformers in a column transformer\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('cat', categorical_transformer, self.categorical_features),\n",
    "                ('num', numerical_transformer, self.numerical_features)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return preprocessor\n",
    "    \n",
    "    def perform_feature_selection(self, X, y, method='hybrid', n_features_to_select=None):\n",
    "        \"\"\"\n",
    "        Perform feature selection using various methods\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature matrix\n",
    "        y : pd.Series\n",
    "            Target variable\n",
    "        method : str\n",
    "            Feature selection method ('rfe', 'model_based', or 'hybrid')\n",
    "        n_features_to_select : int\n",
    "            Number of features to select (if None, automatic selection)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            Selected feature names\n",
    "        \"\"\"\n",
    "        # Create preprocessing pipeline\n",
    "        preprocessor = self.create_preprocessing_pipeline()\n",
    "        \n",
    "        # Transform data\n",
    "        X_processed = preprocessor.fit_transform(X)\n",
    "        \n",
    "        # Get feature names after preprocessing\n",
    "        cat_features = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(self.categorical_features)\n",
    "        all_features = np.concatenate([cat_features, self.numerical_features])\n",
    "        \n",
    "        if method == 'rfe':\n",
    "            # Recursive Feature Elimination with Cross-Validation\n",
    "            estimator = RandomForestClassifier(n_estimators=100, random_state=self.random_state)\n",
    "            selector = RFECV(\n",
    "                estimator=estimator,\n",
    "                step=1,\n",
    "                cv=StratifiedKFold(5, shuffle=True, random_state=self.random_state),\n",
    "                scoring='f1_macro',\n",
    "                min_features_to_select=5\n",
    "            )\n",
    "            selector.fit(X_processed, y)\n",
    "            \n",
    "            # Get selected features\n",
    "            selected_indices = np.where(selector.support_)[0]\n",
    "            selected_features = all_features[selected_indices].tolist()\n",
    "            \n",
    "            # Plot number of features vs. CV score\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.xlabel(\"Number of features selected\")\n",
    "            plt.ylabel(\"Cross-validation score (F1-macro)\")\n",
    "            plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\n",
    "            plt.title(\"Recursive Feature Elimination with Cross-Validation\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./visualizations/rfe_cv_scores.png', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "        elif method == 'model_based':\n",
    "            # Model-based feature selection\n",
    "            estimator = GradientBoostingClassifier(n_estimators=100, random_state=self.random_state)\n",
    "            estimator.fit(X_processed, y)\n",
    "            \n",
    "            # Select features based on importance\n",
    "            selector = SelectFromModel(\n",
    "                estimator=estimator,\n",
    "                threshold='median',\n",
    "                prefit=True\n",
    "            )\n",
    "            \n",
    "            # Get selected features\n",
    "            selected_indices = np.where(selector.get_support())[0]\n",
    "            selected_features = all_features[selected_indices].tolist()\n",
    "            \n",
    "            # Plot feature importances\n",
    "            importances = estimator.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.title(\"Feature Importances\")\n",
    "            plt.bar(range(X_processed.shape[1]), importances[indices], align='center')\n",
    "            plt.xticks(range(X_processed.shape[1]), [all_features[i] for i in indices], rotation=90)\n",
    "            plt.xlim([-1, min(20, X_processed.shape[1])])\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./visualizations/feature_importances.png', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "        elif method == 'hybrid':\n",
    "            # Hybrid approach: first RFE, then model-based selection\n",
    "            # Step 1: RFE to get top 50% features\n",
    "            rfe_estimator = RandomForestClassifier(n_estimators=100, random_state=self.random_state)\n",
    "            rfe_selector = RFECV(\n",
    "                estimator=rfe_estimator,\n",
    "                step=1,\n",
    "                cv=StratifiedKFold(5, shuffle=True, random_state=self.random_state),\n",
    "                scoring='f1_macro',\n",
    "                min_features_to_select=max(5, X_processed.shape[1] // 2)\n",
    "            )\n",
    "            rfe_selector.fit(X_processed, y)\n",
    "            \n",
    "            # Get RFE selected features\n",
    "            rfe_selected_indices = np.where(rfe_selector.support_)[0]\n",
    "            X_rfe_selected = X_processed[:, rfe_selected_indices]\n",
    "            rfe_selected_features = all_features[rfe_selected_indices].tolist()\n",
    "            \n",
    "            # Step 2: Model-based selection on RFE-selected features\n",
    "            model_estimator = GradientBoostingClassifier(n_estimators=100, random_state=self.random_state)\n",
    "            model_estimator.fit(X_rfe_selected, y)\n",
    "            \n",
    "            # Select features based on importance\n",
    "            model_selector = SelectFromModel(\n",
    "                estimator=model_estimator,\n",
    "                threshold='median',\n",
    "                prefit=True\n",
    "            )\n",
    "            \n",
    "            # Get final selected features\n",
    "            model_selected_indices = np.where(model_selector.get_support())[0]\n",
    "            final_selected_indices = rfe_selected_indices[model_selected_indices]\n",
    "            selected_features = all_features[final_selected_indices].tolist()\n",
    "            \n",
    "            # Plot feature importances for final selected features\n",
    "            importances = model_estimator.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.title(\"Feature Importances (Hybrid Selection)\")\n",
    "            plt.bar(range(len(indices)), importances[indices], align='center')\n",
    "            plt.xticks(range(len(indices)), [rfe_selected_features[i] for i in indices], rotation=90)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./visualizations/hybrid_feature_importances.png', dpi=300)\n",
    "            plt.close()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported feature selection method: {method}\")\n",
    "        \n",
    "        print(f\"Selected {len(selected_features)} features using {method} method:\")\n",
    "        print(selected_features)\n",
    "        \n",
    "        # Store selected features\n",
    "        self.selected_features = selected_features\n",
    "        \n",
    "        return selected_features\n",
    "    \n",
    "    def create_model_pipelines(self, preprocessor):\n",
    "        \"\"\"\n",
    "        Create machine learning pipelines with various models\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        preprocessor : ColumnTransformer\n",
    "            Preprocessing pipeline\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of model pipelines\n",
    "        \"\"\"\n",
    "        # Create model pipelines\n",
    "        pipelines = {\n",
    "            'logistic_regression': Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', LogisticRegression(\n",
    "                    multi_class='multinomial', \n",
    "                    solver='lbfgs',\n",
    "                    max_iter=1000,\n",
    "                    random_state=self.random_state\n",
    "                ))\n",
    "            ]),\n",
    "            \n",
    "            'svm': Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', SVC(\n",
    "                    probability=True,\n",
    "                    random_state=self.random_state\n",
    "                ))\n",
    "            ]),\n",
    "            \n",
    "            'knn': Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', KNeighborsClassifier())\n",
    "            ]),\n",
    "            \n",
    "            'random_forest': Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', RandomForestClassifier(\n",
    "                    random_state=self.random_state\n",
    "                ))\n",
    "            ]),\n",
    "            \n",
    "            'gradient_boosting': Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', GradientBoostingClassifier(\n",
    "                    random_state=self.random_state\n",
    "                ))\n",
    "            ]),\n",
    "            \n",
    "            'xgboost': Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', xgb.XGBClassifier(\n",
    "                    objective='multi:softprob',\n",
    "                    random_state=self.random_state\n",
    "                ))\n",
    "            ]),\n",
    "            \n",
    "            'mlp': Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', MLPClassifier(\n",
    "                    hidden_layer_sizes=(100, 50),\n",
    "                    max_iter=1000,\n",
    "                    early_stopping=True,\n",
    "                    random_state=self.random_state\n",
    "                ))\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        return pipelines\n",
    "    \n",
    "    def train_and_tune_models(self, pipelines):\n",
    "        \"\"\"\n",
    "        Train and tune models using grid search\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pipelines : dict\n",
    "            Dictionary of model pipelines\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of trained models\n",
    "        \"\"\"\n",
    "        # Define parameter grids for each model\n",
    "        param_grids = {\n",
    "            'logistic_regression': {\n",
    "                'classifier__C': [0.01, 0.1, 1.0, 10.0],\n",
    "                'classifier__penalty': ['l2', None]\n",
    "            },\n",
    "            \n",
    "            'svm': {\n",
    "                'classifier__C': [0.1, 1.0, 10.0],\n",
    "                'classifier__gamma': ['scale', 'auto'],\n",
    "                'classifier__kernel': ['rbf', 'linear']\n",
    "            },\n",
    "            \n",
    "            'knn': {\n",
    "                'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "                'classifier__weights': ['uniform', 'distance'],\n",
    "                'classifier__metric': ['euclidean', 'manhattan']\n",
    "            },\n",
    "            \n",
    "            'random_forest': {\n",
    "                'classifier__n_estimators': [100, 200],\n",
    "                'classifier__max_depth': [None, 10, 20],\n",
    "                'classifier__min_samples_split': [2, 5],\n",
    "                'classifier__min_samples_leaf': [1, 2]\n",
    "            },\n",
    "            \n",
    "            'gradient_boosting': {\n",
    "                'classifier__n_estimators': [100, 200],\n",
    "                'classifier__learning_rate': [0.01, 0.1],\n",
    "                'classifier__max_depth': [3, 5],\n",
    "                'classifier__min_samples_split': [2, 5]\n",
    "            },\n",
    "            \n",
    "            'xgboost': {\n",
    "                'classifier__n_estimators': [100, 200],\n",
    "                'classifier__learning_rate': [0.01, 0.1],\n",
    "                'classifier__max_depth': [3, 5, 7],\n",
    "                'classifier__min_child_weight': [1, 3, 5]\n",
    "            },\n",
    "            \n",
    "            'mlp': {\n",
    "                'classifier__hidden_layer_sizes': [(50,), (100,), (50, 25), (100, 50)],\n",
    "                'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "                'classifier__learning_rate': ['constant', 'adaptive']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Train and tune models\n",
    "        trained_models = {}\n",
    "        \n",
    "        for name, pipeline in pipelines.items():\n",
    "            print(f\"\\nTraining and tuning {name}...\")\n",
    "            \n",
    "            # Create grid search\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=pipeline,\n",
    "                param_grid=param_grids[name],\n",
    "                cv=StratifiedKFold(5, shuffle=True, random_state=self.random_state),\n",
    "                scoring='f1_macro',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Fit grid search\n",
    "            grid_search.fit(self.X_train, self.y_train)\n",
    "            \n",
    "            # Get best model\n",
    "            best_model = grid_search.best_estimator_\n",
    "            \n",
    "            # Store best model\n",
    "            trained_models[name] = best_model\n",
    "            \n",
    "            # Print best parameters\n",
    "            print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "            print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "            \n",
    "            # Save model\n",
    "            joblib.dump(best_model, f'./models/{name}_model.pkl')\n",
    "        \n",
    "        # Store trained models\n",
    "        self.models = trained_models\n",
    "        \n",
    "        return trained_models\n",
    "    \n",
    "    def train_neural_network(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Train a deep neural network for CKD classification\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : pd.DataFrame\n",
    "            Training features\n",
    "        y_train : pd.Series\n",
    "            Training target\n",
    "        X_val : pd.DataFrame\n",
    "            Validation features\n",
    "        y_val : pd.Series\n",
    "            Validation target\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tf.keras.Model\n",
    "            Trained neural network model\n",
    "        \"\"\"\n",
    "        # Create preprocessing pipeline\n",
    "        preprocessor = self.create_preprocessing_pipeline()\n",
    "        \n",
    "        # Transform data\n",
    "        X_train_processed = preprocessor.fit_transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "        \n",
    "        # Convert target to categorical\n",
    "        # First, map string labels to integers\n",
    "        label_mapping = {label: i for i, label in enumerate(sorted(y_train.unique()))}\n",
    "        y_train_int = y_train.map(label_mapping)\n",
    "        y_val_int = y_val.map(label_mapping)\n",
    "        \n",
    "        # Then convert to one-hot encoding\n",
    "        y_train_cat = to_categorical(y_train_int)\n",
    "        y_val_cat = to_categorical(y_val_int)\n",
    "        \n",
    "        # Get input shape\n",
    "        input_shape = X_train_processed.shape[1]\n",
    "        \n",
    "        # Create model\n",
    "        model = Sequential([\n",
    "            # Input layer\n",
    "            Dense(256, activation='relu', input_shape=(input_shape,)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Hidden layers\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            Dense(64, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(len(label_mapping), activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Create early stopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_processed, y_train_cat,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_val_processed, y_val_cat),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('./visualizations/neural_network_training_history.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Save model\n",
    "        model.save('./models/neural_network_model')\n",
    "        \n",
    "        # Save preprocessor\n",
    "        joblib.dump(preprocessor, './models/neural_network_preprocessor.pkl')\n",
    "        \n",
    "        # Save label mapping\n",
    "        joblib.dump(label_mapping, './models/neural_network_label_mapping.pkl')\n",
    "        \n",
    "        # Store model and related objects\n",
    "        self.nn_model = model\n",
    "        self.nn_preprocessor = preprocessor\n",
    "        self.nn_label_mapping = label_mapping\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_voting_ensemble(self, models, voting='soft'):\n",
    "        \"\"\"\n",
    "        Create a voting ensemble from multiple models\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        models : dict\n",
    "            Dictionary of trained models\n",
    "        voting : str\n",
    "            Voting strategy ('hard' or 'soft')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        sklearn.ensemble.VotingClassifier\n",
    "            Voting ensemble model\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import VotingClassifier\n",
    "        \n",
    "        # Create list of (name, model) tuples for VotingClassifier\n",
    "        estimators = [(name, model) for name, model in models.items()]\n",
    "        \n",
    "        # Create voting ensemble\n",
    "        ensemble = VotingClassifier(\n",
    "            estimators=estimators,\n",
    "            voting=voting\n",
    "        )\n",
    "        \n",
    "        # Fit ensemble\n",
    "        ensemble.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Save ensemble\n",
    "        joblib.dump(ensemble, f'./models/voting_ensemble_{voting}.pkl')\n",
    "        \n",
    "        # Store ensemble\n",
    "        self.ensemble = ensemble\n",
    "        \n",
    "        return ensemble\n",
    "    \n",
    "    def evaluate_model(self, model, X, y, model_name):\n",
    "        \"\"\"\n",
    "        Evaluate a model on given data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : sklearn.base.BaseEstimator\n",
    "            Trained model\n",
    "        X : pd.DataFrame\n",
    "            Feature matrix\n",
    "        y : pd.Series\n",
    "            Target variable\n",
    "        model_name : str\n",
    "            Name of the model\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        f1_macro = f1_score(y, y_pred, average='macro')\n",
    "        f1_weighted = f1_score(y, y_pred, average='weighted')\n",
    "        \n",
    "        # Generate classification report\n",
    "        report = classification_report(y, y_pred, output_dict=True)\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_macro': f1_macro,\n",
    "            'f1_weighted': f1_weighted,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nEvaluation results for {model_name}:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "        print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y, y_pred))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=sorted(y.unique()),\n",
    "                   yticklabels=sorted(y.unique()))\n",
    "        plt.title(f'Confusion Matrix - {model_name}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'./visualizations/confusion_matrix_{model_name}.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_all_models(self):\n",
    "        \"\"\"\n",
    "        Evaluate all trained models on validation and test sets\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of evaluation results for all models\n",
    "        \"\"\"\n",
    "        # Initialize results dictionary\n",
    "        all_results = {}\n",
    "        \n",
    "        # Evaluate each model\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nEvaluating {name}...\")\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_results = self.evaluate_model(model, self.X_val, self.y_val, f\"{name}_val\")\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_results = self.evaluate_model(model, self.X_test, self.y_test, f\"{name}_test\")\n",
    "            \n",
    "            # Store results\n",
    "            all_results[name] = {\n",
    "                'validation': val_results,\n",
    "                'test': test_results\n",
    "            }\n",
    "        \n",
    "        # Evaluate ensemble if available\n",
    "        if hasattr(self, 'ensemble'):\n",
    "            print(\"\\nEvaluating voting ensemble...\")\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_results = self.evaluate_model(self.ensemble, self.X_val, self.y_val, \"ensemble_val\")\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_results = self.evaluate_model(self.ensemble, self.X_test, self.y_test, \"ensemble_test\")\n",
    "            \n",
    "            # Store results\n",
    "            all_results['ensemble'] = {\n",
    "                'validation': val_results,\n",
    "                'test': test_results\n",
    "            }\n",
    "        \n",
    "        # Evaluate neural network if available\n",
    "        if hasattr(self, 'nn_model'):\n",
    "            print(\"\\nEvaluating neural network...\")\n",
    "            \n",
    "            # Transform data\n",
    "            X_val_processed = self.nn_preprocessor.transform(self.X_val)\n",
    "            X_test_processed = self.nn_preprocessor.transform(self.X_test)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_val_pred_proba = self.nn_model.predict(X_val_processed)\n",
    "            y_test_pred_proba = self.nn_model.predict(X_test_processed)\n",
    "            \n",
    "            # Convert probabilities to class indices\n",
    "            y_val_pred_idx = np.argmax(y_val_pred_proba, axis=1)\n",
    "            y_test_pred_idx = np.argmax(y_test_pred_proba, axis=1)\n",
    "            \n",
    "            # Convert indices back to original labels\n",
    "            reverse_mapping = {v: k for k, v in self.nn_label_mapping.items()}\n",
    "            y_val_pred = np.array([reverse_mapping[idx] for idx in y_val_pred_idx])\n",
    "            y_test_pred = np.array([reverse_mapping[idx] for idx in y_test_pred_idx])\n",
    "            \n",
    "            # Calculate metrics for validation set\n",
    "            val_accuracy = accuracy_score(self.y_val, y_val_pred)\n",
    "            val_f1_macro = f1_score(self.y_val, y_val_pred, average='macro')\n",
    "            val_f1_weighted = f1_score(self.y_val, y_val_pred, average='weighted')\n",
    "            val_report = classification_report(self.y_val, y_val_pred, output_dict=True)\n",
    "            val_cm = confusion_matrix(self.y_val, y_val_pred)\n",
    "            \n",
    "            # Calculate metrics for test set\n",
    "            test_accuracy = accuracy_score(self.y_test, y_test_pred)\n",
    "            test_f1_macro = f1_score(self.y_test, y_test_pred, average='macro')\n",
    "            test_f1_weighted = f1_score(self.y_test, y_test_pred, average='weighted')\n",
    "            test_report = classification_report(self.y_test, y_test_pred, output_dict=True)\n",
    "            test_cm = confusion_matrix(self.y_test, y_test_pred)\n",
    "            \n",
    "            # Store results\n",
    "            all_results['neural_network'] = {\n",
    "                'validation': {\n",
    "                    'accuracy': val_accuracy,\n",
    "                    'f1_macro': val_f1_macro,\n",
    "                    'f1_weighted': val_f1_weighted,\n",
    "                    'classification_report': val_report,\n",
    "                    'confusion_matrix': val_cm\n",
    "                },\n",
    "                'test': {\n",
    "                    'accuracy': test_accuracy,\n",
    "                    'f1_macro': test_f1_macro,\n",
    "                    'f1_weighted': test_f1_weighted,\n",
    "                    'classification_report': test_report,\n",
    "                    'confusion_matrix': test_cm\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            print(\"\\nNeural Network Validation Results:\")\n",
    "            print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"F1 Score (Macro): {val_f1_macro:.4f}\")\n",
    "            print(f\"F1 Score (Weighted): {val_f1_weighted:.4f}\")\n",
    "            \n",
    "            print(\"\\nNeural Network Test Results:\")\n",
    "            print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "            print(f\"F1 Score (Macro): {test_f1_macro:.4f}\")\n",
    "            print(f\"F1 Score (Weighted): {test_f1_weighted:.4f}\")\n",
    "            \n",
    "            # Plot confusion matrices\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(val_cm, annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=sorted(self.y_val.unique()),\n",
    "                       yticklabels=sorted(self.y_val.unique()))\n",
    "            plt.title('Confusion Matrix - Neural Network (Validation)')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./visualizations/confusion_matrix_neural_network_val.png', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=sorted(self.y_test.unique()),\n",
    "                       yticklabels=sorted(self.y_test.unique()))\n",
    "            plt.title('Confusion Matrix - Neural Network (Test)')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./visualizations/confusion_matrix_neural_network_test.png', dpi=300)\n",
    "            plt.close()\n",
    "        \n",
    "        # Store all results\n",
    "        self.results = all_results\n",
    "        \n",
    "        # Save results\n",
    "        joblib.dump(all_results, './results/all_evaluation_results.pkl')\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"\n",
    "        Compare performance of all models\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            DataFrame with model comparison\n",
    "        \"\"\"\n",
    "        # Extract test set results for all models\n",
    "        model_names = []\n",
    "        accuracies = []\n",
    "        f1_macros = []\n",
    "        f1_weighteds = []\n",
    "        \n",
    "        for name, results in self.results.items():\n",
    "            model_names.append(name)\n",
    "            accuracies.append(results['test']['accuracy'])\n",
    "            f1_macros.append(results['test']['f1_macro'])\n",
    "            f1_weighteds.append(results['test']['f1_weighted'])\n",
    "        \n",
    "        # Create comparison DataFrame\n",
    "        comparison = pd.DataFrame({\n",
    "            'Model': model_names,\n",
    "            'Accuracy': accuracies,\n",
    "            'F1 (Macro)': f1_macros,\n",
    "            'F1 (Weighted)': f1_weighteds\n",
    "        })\n",
    "        \n",
    "        # Sort by F1 (Macro) in descending order\n",
    "        comparison = comparison.sort_values('F1 (Macro)', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Save comparison\n",
    "        comparison.to_csv('./results/model_comparison.csv', index=False)\n",
    "        \n",
    "        # Plot comparison\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.25\n",
    "        \n",
    "        plt.bar(x - width, accuracies, width, label='Accuracy')\n",
    "        plt.bar(x, f1_macros, width, label='F1 (Macro)')\n",
    "        plt.bar(x + width, f1_weighteds, width, label='F1 (Weighted)')\n",
    "        \n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Model Performance Comparison')\n",
    "        plt.xticks(x, model_names, rotation=45)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('./visualizations/model_comparison.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Print comparison\n",
    "        print(\"\\nModel Comparison:\")\n",
    "        print(comparison)\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def extract_feature_importances(self):\n",
    "        \"\"\"\n",
    "        Extract feature importances from tree-based models\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of feature importances for each model\n",
    "        \"\"\"\n",
    "        # Initialize feature importances dictionary\n",
    "        feature_importances = {}\n",
    "        \n",
    "        # Extract feature importances from tree-based models\n",
    "        tree_based_models = ['random_forest', 'gradient_boosting', 'xgboost']\n",
    "        \n",
    "        for name in tree_based_models:\n",
    "            if name in self.models:\n",
    "                model = self.models[name]\n",
    "                \n",
    "                # Get preprocessor and classifier\n",
    "                preprocessor = model.named_steps['preprocessor']\n",
    "                classifier = model.named_steps['classifier']\n",
    "                \n",
    "                # Get feature names after preprocessing\n",
    "                cat_features = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(self.categorical_features)\n",
    "                all_features = np.concatenate([cat_features, self.numerical_features])\n",
    "                \n",
    "                # Get feature importances\n",
    "                importances = classifier.feature_importances_\n",
    "                \n",
    "                # Create DataFrame\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'Feature': all_features,\n",
    "                    'Importance': importances\n",
    "                })\n",
    "                \n",
    "                # Sort by importance in descending order\n",
    "                importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "                \n",
    "                # Store feature importances\n",
    "                feature_importances[name] = importance_df\n",
    "                \n",
    "                # Save feature importances\n",
    "                importance_df.to_csv(f'./results/feature_importances_{name}.csv', index=False)\n",
    "                \n",
    "                # Plot feature importances\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "                plt.title(f'Top 20 Feature Importances - {name}')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'./visualizations/feature_importances_{name}.png', dpi=300)\n",
    "                plt.close()\n",
    "        \n",
    "        # Store feature importances\n",
    "        self.feature_importances = feature_importances\n",
    "        \n",
    "        return feature_importances\n",
    "    \n",
    "    def analyze_misclassifications(self, model_name='ensemble'):\n",
    "       \n",
    "        # Get model\n",
    "        if model_name == 'ensemble' and hasattr(self, 'ensemble'):\n",
    "            model = self.ensemble\n",
    "        elif model_name == 'neural_network' and hasattr(self, 'nn_model'):\n",
    "            # For neural network, we need special handling\n",
    "            # Transform data\n",
    "            X_test_processed = self.nn_preprocessor.transform(self.X_test)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_test_pred_proba = self.nn_model.predict(X_test_processed)\n",
    "            \n",
    "            # Convert probabilities to class indices\n",
    "            y_test_pred_idx = np.argmax(y_test_pred_proba, axis=1)\n",
    "            \n",
    "            # Convert indices back to original labels\n",
    "            reverse_mapping = {v: k for k, v in self.nn_label_mapping.items()}\n",
    "            y_pred = np.array([reverse_mapping[idx] for idx in y_test_pred_idx])\n",
    "        elif model_name in self.models:\n",
    "            model = self.models[model_name]\n",
    "        else:\n",
    "            raise ValueError(f\"Model '{model_name}' not found\")\n",
    "        \n",
    "        # Make predictions (if not already done for neural network)\n",
    "        if model_name != 'neural_network':\n",
    "            y_pred = model.predict(self.X_test)\n",
    "        \n",
    "        # Identify misclassified samples\n",
    "        misclassified = self.y_test != y_pred\n",
    "        \n",
    "        # Create DataFrame with misclassified samples\n",
    "        misclassified_df = pd.DataFrame({\n",
    "            'True_Stage': self.y_test[misclassified].values,\n",
    "            'Predicted_Stage': y_pred[misclassified],\n",
    "            'GFR': self.X_test.loc[misclassified, 'GFR'].values,\n",
    "            'ACR': self.X_test.loc[misclassified, 'ACR'].values,\n",
    "            'Age': self.X_test.loc[misclassified, 'Age'].values,\n",
    "            'Gender': self.X_test.loc[misclassified, 'Gender'].values,\n",
    "            'Creatinine': self.X_test.loc[misclassified, 'Creatinine'].values,\n",
    "            'Cystatin_C': self.X_test.loc[misclassified, 'Cystatin_C'].values\n",
    "        })\n",
    "        \n",
    "        # Save misclassified samples\n",
    "        misclassified_df.to_csv(f'./results/misclassified_{model_name}.csv', index=False)\n",
    "        \n",
    "        # Analyze misclassifications by stage\n",
    "        misclass_by_stage = pd.crosstab(\n",
    "            self.y_test[misclassified], \n",
    "            y_pred[misclassified],\n",
    "            rownames=['True'],\n",
    "            colnames=['Predicted']\n",
    "        )\n",
    "        \n",
    "        # Save misclassifications by stage\n",
    "        misclass_by_stage.to_csv(f'./results/misclassifications_by_stage_{model_name}.csv')\n",
    "        \n",
    "        # Plot misclassifications by stage\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(misclass_by_stage, annot=True, fmt='d', cmap='YlOrRd')\n",
    "        plt.title(f'Misclassifications by Stage - {model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'./visualizations/misclassifications_by_stage_{model_name}.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Analyze GFR distribution of misclassified samples\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.boxplot(x='True_Stage', y='GFR', hue='Predicted_Stage', data=misclassified_df)\n",
    "        plt.title(f'GFR Distribution of Misclassified Samples - {model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'./visualizations/misclassified_gfr_distribution_{model_name}.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nMisclassification Analysis for {model_name}:\")\n",
    "        print(f\"Total misclassified samples: {misclassified.sum()} out of {len(self.y_test)} ({misclassified.sum() / len(self.y_test) * 100:.2f}%)\")\n",
    "        print(\"\\nMisclassifications by Stage:\")\n",
    "        print(misclass_by_stage)\n",
    "        \n",
    "        return misclassified_df\n",
    "    \n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"\n",
    "        Generate a summary report of all analyses\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Summary report\n",
    "        \"\"\"\n",
    "        # Create summary report\n",
    "        report = \"# CKD Classification Summary Report\\n\\n\"\n",
    "        \n",
    "        # Add dataset information\n",
    "        report += \"## Dataset Information\\n\\n\"\n",
    "        report += f\"- Training set: {self.X_train.shape[0]} samples (60%)\\n\"\n",
    "        report += f\"- Validation set: {self.X_val.shape[0]} samples (20%)\\n\"\n",
    "        report += f\"- Test set: {self.X_test.shape[0]} samples (20%)\\n\"\n",
    "        report += f\"- Features: {self.X_train.shape[1]}\\n\"\n",
    "        report += f\"- Classes: {', '.join(self.classes)}\\n\\n\"\n",
    "        \n",
    "        # Add feature selection information if available\n",
    "        if hasattr(self, 'selected_features'):\n",
    "            report += \"## Feature Selection\\n\\n\"\n",
    "            report += f\"- Selected {len(self.selected_features)} features\\n\"\n",
    "            report += f\"- Top 10 features: {', '.join(self.selected_features[:10])}\\n\\n\"\n",
    "        \n",
    "        # Add model comparison\n",
    "        if hasattr(self, 'results'):\n",
    "            report += \"## Model Comparison\\n\\n\"\n",
    "            report += \"| Model | Accuracy | F1 (Macro) | F1 (Weighted) |\\n\"\n",
    "            report += \"|-------|----------|------------|---------------|\\n\"\n",
    "            \n",
    "            for name, results in self.results.items():\n",
    "                accuracy = results['test']['accuracy']\n",
    "                f1_macro = results['test']['f1_macro']\n",
    "                f1_weighted = results['test']['f1_weighted']\n",
    "                \n",
    "                report += f\"| {name} | {accuracy:.4f} | {f1_macro:.4f} | {f1_weighted:.4f} |\\n\"\n",
    "            \n",
    "            report += \"\\n\"\n",
    "        \n",
    "        # Add feature importance information if available\n",
    "        if hasattr(self, 'feature_importances') and self.feature_importances:\n",
    "            report += \"## Feature Importances\\n\\n\"\n",
    "            \n",
    "            for name, importances in self.feature_importances.items():\n",
    "                report += f\"### {name}\\n\\n\"\n",
    "                report += \"| Feature | Importance |\\n\"\n",
    "                report += \"|---------|------------|\\n\"\n",
    "                \n",
    "                for _, row in importances.head(10).iterrows():\n",
    "                    feature = row['Feature']\n",
    "                    importance = row['Importance']\n",
    "                    \n",
    "                    report += f\"| {feature} | {importance:.4f} |\\n\"\n",
    "                \n",
    "                report += \"\\n\"\n",
    "        \n",
    "        # Add misclassification analysis if available\n",
    "        if hasattr(self, 'results'):\n",
    "            report += \"## Misclassification Analysis\\n\\n\"\n",
    "            \n",
    "            # Get best model based on F1 (Macro)\n",
    "            best_model = max(self.results.items(), key=lambda x: x[1]['test']['f1_macro'])[0]\n",
    "            \n",
    "            report += f\"### {best_model} (Best Model)\\n\\n\"\n",
    "            \n",
    "            # Get confusion matrix\n",
    "            cm = self.results[best_model]['test']['confusion_matrix']\n",
    "            \n",
    "            report += \"Confusion Matrix:\\n\\n\"\n",
    "            report += \"```\\n\"\n",
    "            report += str(cm)\n",
    "            report += \"\\n```\\n\\n\"\n",
    "            \n",
    "            # Get classification report\n",
    "            class_report = self.results[best_model]['test']['classification_report']\n",
    "            \n",
    "            report += \"Classification Report:\\n\\n\"\n",
    "            report += \"| Class | Precision | Recall | F1-Score | Support |\\n\"\n",
    "            report += \"|-------|-----------|--------|----------|--------|\\n\"\n",
    "            \n",
    "            for class_name, metrics in class_report.items():\n",
    "                if class_name in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                    continue\n",
    "                \n",
    "                precision = metrics['precision']\n",
    "                recall = metrics['recall']\n",
    "                f1 = metrics['f1-score']\n",
    "                support = metrics['support']\n",
    "                \n",
    "                report += f\"| {class_name} | {precision:.4f} | {recall:.4f} | {f1:.4f} | {support} |\\n\"\n",
    "            \n",
    "            report += \"\\n\"\n",
    "        \n",
    "        # Add conclusion\n",
    "        report += \"## Conclusion\\n\\n\"\n",
    "        \n",
    "        if hasattr(self, 'results'):\n",
    "            # Get best model based on F1 (Macro)\n",
    "            best_model = max(self.results.items(), key=lambda x: x[1]['test']['f1_macro'])[0]\n",
    "            best_f1 = self.results[best_model]['test']['f1_macro']\n",
    "            \n",
    "            report += f\"The best performing model is **{best_model}** with a macro F1 score of {best_f1:.4f} on the test set.\\n\\n\"\n",
    "        \n",
    "        report += \"Key findings:\\n\\n\"\n",
    "        report += \"1. The model successfully classifies CKD stages with high accuracy\\n\"\n",
    "        report += \"2. GFR and creatinine are the most important features for classification\\n\"\n",
    "        report += \"3. Misclassifications primarily occur between adjacent stages\\n\"\n",
    "        report += \"4. Advanced models like ensemble methods and neural networks provide the best performance\\n\"\n",
    "        \n",
    "        # Save report\n",
    "        with open('./results/summary_report.md', 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        return report\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the CKD classification pipeline\n",
    "    \"\"\"\n",
    "    print(\"Starting CKD Classification Pipeline...\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = CKDClassificationPipeline()\n",
    "    \n",
    "    # Load data\n",
    "    pipeline.load_data(\n",
    "        train_path='ckd_train_data.csv',\n",
    "        val_path='ckd_val_data.csv',\n",
    "        test_path='ckd_test_data.csv'\n",
    "    )\n",
    "    \n",
    "    # Perform feature selection\n",
    "    pipeline.perform_feature_selection(\n",
    "        pipeline.X_train, \n",
    "        pipeline.y_train,\n",
    "        method='hybrid'\n",
    "    )\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = pipeline.create_preprocessing_pipeline()\n",
    "    \n",
    "    # Create model pipelines\n",
    "    model_pipelines = pipeline.create_model_pipelines(preprocessor)\n",
    "    \n",
    "    # Train and tune models\n",
    "    trained_models = pipeline.train_and_tune_models(model_pipelines)\n",
    "    \n",
    "    # Train neural network\n",
    "    pipeline.train_neural_network(\n",
    "        pipeline.X_train,\n",
    "        pipeline.y_train,\n",
    "        pipeline.X_val,\n",
    "        pipeline.y_val\n",
    "    )\n",
    "    \n",
    "    # Create voting ensemble\n",
    "    pipeline.create_voting_ensemble(\n",
    "        {name: model for name, model in trained_models.items() \n",
    "         if name in ['random_forest', 'gradient_boosting', 'xgboost']}\n",
    "    )\n",
    "    \n",
    "    # Evaluate all models\n",
    "    pipeline.evaluate_all_models()\n",
    "    \n",
    "    # Compare models\n",
    "    pipeline.compare_models()\n",
    "    \n",
    "    # Extract feature importances\n",
    "    pipeline.extract_feature_importances()\n",
    "    \n",
    "    # Analyze misclassifications\n",
    "    pipeline.analyze_misclassifications(model_name='ensemble')\n",
    "    \n",
    "    # Generate summary report\n",
    "    pipeline.generate_summary_report()\n",
    "    \n",
    "    print(\"\\nCKD Classification Pipeline completed successfully!\")\n",
    "    print(\"Results and visualizations saved to ./results/ and ./visualizations/ directories\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5beb961-6505-4b18-b0f3-1e25ca7c94d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
